Anthropic just released  Model Context Protocol (MCP), a new standard for connecting AI assistants to the systems where data lives, including content repositories, business tools, and development environments. The Model Context Protocol is an open standard that enables developers to build secure, two-way connections between their data sources and AI-powered tools. The architecture is straightforward: developers can either expose their data through MCP servers or build AI applications (MCP clients) that connect to these servers.

Model Context Protocol (MCP), is heavily inspired by Microsoft's Language Server Protocol (LSP) used in IDEs.

# MCP Protocol

## Lifecycle

The Model Context Protocol (MCP) defines a rigorous lifecycle for client-server connections that ensures proper capability negotiation and state management.

3 phases:
1. Initialization: Capability negotiation and protocol version agreement
2. Operation: Normal protocol communication
3. Shutdown: Graceful termination of the connection


### Initialization 
The initialization phase MUST be the first interaction between client and server. During this phase, the client and server:
- Establish protocol version compatibility
- Exchange and negotiate capabilities
- Share implementation details

The client MUST initiate this phase by sending an initialize request containing:
- Protocol version supported
- Client capabilities
- Client implementation information

### Operation 
During the operation phase, the client and server exchange messages according to the negotiated capabilities.

Both parties SHOULD:
- Respect the negotiated protocol version
- Only use capabilities that were successfully negotiated

### Shutdown 
During the shutdown phase, one side (usually the client) cleanly terminates the protocol connection. No specific shutdown messages are defined—instead, the underlying transport mechanism should be used to signal connection termination:

#### stdio 
For the stdio transport, the client SHOULD initiate shutdown by:

1. First, closing the input stream to the child process (the server)
2. Waiting for the server to exit, or sending SIGTERM if the server does not exit within a reasonable time
3. Sending SIGKILL if the server does not exit within a reasonable time after SIGTERM

The server MAY initiate shutdown by closing its output stream to the client and exiting.

#### HTTP with SSE

For HTTP transports, shutdown is indicated by closing the associated HTTP connection(s).


## Transports

2 standard transport mechanisms for client-server communication:

1. stdio, communication over standard in and standard out
2. HTTP with Server-Sent Events (SSE)

### stdio

- The client launches the MCP server as a subprocess.
- The server receives JSON-RPC messages on its standard input (stdin) and writes responses to its standard output (stdout).
- Messages are delimited by newlines, and MUST NOT contain embedded newlines.
- The server MAY write UTF-8 strings to its standard error (stderr) for logging purposes. Clients MAY capture, forward, or ignore this logging.
- The server MUST NOT write anything to its stdout that is not a valid MCP message.
- The client MUST NOT write anything to the server’s stdin that is not a valid MCP message.

### HTTP with SSE

In the SSE transport, the server operates as an independent process that can handle multiple client connections.

The server MUST provide two endpoints:
1. An SSE endpoint, for clients to establish a connection and receive messages from the server
2. A regular HTTP POST endpoint for clients to send messages to the server

When a client connects, the server MUST send an endpoint event containing a URI for the client to use for sending messages. All subsequent client messages MUST be sent as HTTP POST requests to this endpoint.

Server messages are sent as SSE message events, with the message content encoded as JSON in the event data.

------------------

# General Guidelines for writing Rust code

- Write clear, concise, and idiomatic Rust code with accurate examples.
- Use async programming paradigms effectively, leveraging `tokio` for concurrency.
- Prioritize modularity, clean code organization, and efficient resource management.
- Use expressive variable names that convey intent (e.g., `is_ready`, `has_data`).
- Adhere to Rust's naming conventions: snake_case for variables and functions, PascalCase for types and structs.
- Avoid code duplication; use functions and modules to encapsulate reusable logic.
- Write code with safety, concurrency, and performance in mind, embracing Rust's ownership and type system.

Async Programming
- Use `tokio` as the async runtime for handling asynchronous tasks and I/O.
- Implement async functions using `async fn` syntax.
- Leverage `tokio::spawn` for task spawning and concurrency.
- Use `tokio::select!` for managing multiple async tasks and cancellations.
- Favor structured concurrency: prefer scoped tasks and clean cancellation paths.
- Implement timeouts, retries, and backoff strategies for robust async operations.

Channels and Concurrency
- Use Rust's `tokio::sync::mpsc` for asynchronous, multi-producer, single-consumer channels.
- Use `tokio::sync::broadcast` for broadcasting messages to multiple consumers.
- Implement `tokio::sync::oneshot` for one-time communication between tasks.
- Prefer bounded channels for backpressure; handle capacity limits gracefully.
- Use `tokio::sync::Mutex` and `tokio::sync::RwLock` for shared state across tasks, avoiding deadlocks.

Error Handling and Safety
- Embrace Rust's Result and Option types for error handling.
- Use `?` operator to propagate errors in async functions.
- Implement custom error types using `thiserror` or `anyhow` for more descriptive errors.
- Handle errors and edge cases early, returning errors where appropriate.
- Use `.await` responsibly, ensuring safe points for context switching.

Testing
- Write unit tests with `tokio::test` for async tests.
- Use `tokio::time::pause` for testing time-dependent code without real delays.
- Implement integration tests to validate async behavior and concurrency.
- Use mocks and fakes for external dependencies in tests.

Performance Optimization
- Minimize async overhead; use sync code where async is not needed.
- Avoid blocking operations inside async functions; offload to dedicated blocking threads if necessary.
- Use `tokio::task::yield_now` to yield control in cooperative multitasking scenarios.
- Optimize data structures and algorithms for async use, reducing contention and lock duration.
- Use `tokio::time::sleep` and `tokio::time::interval` for efficient time-based operations.

Key Conventions
1. Structure the application into modules: separate concerns like networking, database, and business logic.
2. Use environment variables for configuration management (e.g., `dotenv` crate).
3. Ensure code is well-documented with inline comments and Rustdoc.

Async Ecosystem
- Use `tokio` for async runtime and task management.
- Leverage `hyper` or `reqwest` for async HTTP requests.
- Use `serde` for serialization/deserialization.


## Tower crate

- Tower is a library of modular and reusable components for building networking clients and servers.
- Tower provides a simple core abstraction, the Service trait, which represents an asynchronous function taking a request and returning either a response or an error. This abstraction can be used to model both clients and servers.
```rust
pub trait Service<Request> {
    type Response;
    type Error;
    type Future: Future<Output = Result<Self::Response, Self::Error>>;

    // Required methods
    fn poll_ready(
        &mut self,
        cx: &mut Context<'_>,
    ) -> Poll<Result<(), Self::Error>>;
    fn call(&mut self, req: Request) -> Self::Future;
}
```
- Generic components, like timeout, rate limiting, and load balancing, can be modeled as Services that wrap some inner service and apply additional behavior before or after the inner service is called. This allows implementing these components in a protocol-agnostic, composable way. Typically, such services are referred to as middleware.
- An additional abstraction, the Layer trait, is used to compose middleware with Services. If a Service can be thought of as an asynchronous function from a request type to a response type, a Layer is a function taking a Service of one type and returning a Service of a different type. The ServiceBuilder type is used to add middleware to a service by composing it with multiple Layers.

Here is an example of using tower service with timeout:
```
use tokio::time::{sleep, Duration};
use tower::{Service, ServiceBuilder};
use std::task::{Context, Poll};
use futures::future::{BoxFuture, FutureExt};

// Define a simple service that takes some time to respond
struct SlowService;

impl Service<String> for SlowService {
    type Response = String;
    type Error = &'static str;
    type Future = BoxFuture<'static, Result<Self::Response, Self::Error>>;

    fn poll_ready(&mut self, _cx: &mut Context<'_>) -> Poll<Result<(), Self::Error>> {
        Poll::Ready(Ok(()))
    }

    fn call(&mut self, req: String) -> Self::Future {
        println!("Processing request: {}", req);

        // Use an async block to create the future
        async move {
            // Simulate a slow response
            sleep(Duration::from_secs(3)).await;
            Ok(format!("Processed: {}", req))
        }
        .boxed() // Convert the future into a BoxFuture
    }
}

#[tokio::main]
async fn main() {
    // Create the base service
    let service = SlowService;

    // Wrap the service with a timeout layer
    let timeout_service = ServiceBuilder::new()
        .timeout(Duration::from_secs(1))
        .service(service);

    // Use the timeout-wrapped service
    let mut svc = timeout_service;

    match svc.call("Hello Tower!".to_string()).await {
        Ok(response) => println!("Response: {}", response),
        Err(err) => {
            if let Some(_elapsed) = err.downcast_ref::<tower::timeout::error::Elapsed>() {
                println!("Error: Timed out");
            } else {
                println!("Error: {:?}", err);
            }
        }
    }
}
```

## Actors in Tokio

- An actor is a self-contained task (usually spawned by an async runtime like Tokio) that runs independently and communicates with other parts of the system by sending and receiving messages.
    - Each actor encapsulates some resource or job.
    - Other tasks/actors access it by sending messages rather than by directly sharing data.

- An actor is split into two parts:
    1. The Task – A run function or method that processes incoming messages in a loop, shutting down gracefully when no more messages can be received.
    2. The Handle – A struct that owns a mpsc::Sender (or other channel) and exposes async methods to send messages to the actor. 

A minimal actor example might look like this:
```
use tokio::sync::{oneshot, mpsc};

// Message type(s) the actor can receive.
enum ActorMessage {
    GetUniqueId {
        respond_to: oneshot::Sender<u32>,
    },
}

// The actor’s internal state and logic.
struct MyActor {
    receiver: mpsc::Receiver<ActorMessage>,
    next_id: u32,
}

impl MyActor {
    fn new(receiver: mpsc::Receiver<ActorMessage>) -> Self {
        Self { receiver, next_id: 0 }
    }

    fn handle_message(&mut self, msg: ActorMessage) {
        match msg {
            ActorMessage::GetUniqueId { respond_to } => {
                self.next_id += 1;
                let _ = respond_to.send(self.next_id); // Ignore send errors
            }
        }
    }
}

// The actor’s main event loop.
async fn run_my_actor(mut actor: MyActor) {
    while let Some(msg) = actor.receiver.recv().await {
        actor.handle_message(msg);
    }
}

// The "handle": provides methods to send messages to the actor.
#[derive(Clone)]
struct MyActorHandle {
    sender: mpsc::Sender<ActorMessage>,
}

impl MyActorHandle {
    // Creates both the sender and the spawned actor task.
    pub fn new() -> Self {
        let (sender, receiver) = mpsc::channel(8);
        let actor = MyActor::new(receiver);
        tokio::spawn(run_my_actor(actor));

        Self { sender }
    }

    // Example request-response method.
    pub async fn get_unique_id(&self) -> u32 {
        let (send, recv) = oneshot::channel();
        let msg = ActorMessage::GetUniqueId { respond_to: send };
        let _ = self.sender.send(msg).await; // Sends the message to the actor
        recv.await.expect("Actor task was killed")
    }
}
```

